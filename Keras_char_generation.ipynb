{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leena/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/leena/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/leena/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/leena/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Activation\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import LambdaCallback, ModelCheckpoint\n",
    "import random\n",
    "import sys\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taking first 10000 character from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text length in number of characters: 42479302\n",
      "head of text:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = open('/home/leena/Downloads/description_sorted.txt').read()\n",
    "print('text length in number of characters:', len(text))\n",
    "\n",
    "text = text[:15000]\n",
    "print('head of text:')\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#text = text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of characters: 72\n",
      "['\\t', '\\n', ' ', '%', '&', \"'\", '(', ')', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', 'A', 'B', 'C', 'D', 'F', 'G', 'I', 'K', 'L', 'M', 'N', 'O', 'R', 'S', 'T', 'U', 'W', 'Y', '\\\\', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\xa0']\n"
     ]
    }
   ],
   "source": [
    "#A set is an unordered collection with no duplicate elements.\n",
    "#conver back to list, sorts alphanumerically\n",
    "#list of all unique chars\n",
    "chars = sorted(list(set(text)))\n",
    "\n",
    "char_size = len(chars)\n",
    "print('number of characters:', char_size)\n",
    "print(chars)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the book is loaded, we must prepare the data for modeling by the neural network. We cannot model the \n",
    "characters directly, instead we must convert the characters to integers.\n",
    "\n",
    "We can do this easily by first creating a set of all of the distinct characters in the book, then creating a map \n",
    "of each character to a unique integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\t': 0, '\\n': 1, ' ': 2, '%': 3, '&': 4, \"'\": 5, '(': 6, ')': 7, ',': 8, '-': 9, '.': 10, '/': 11, '0': 12, '1': 13, '2': 14, '3': 15, '4': 16, '5': 17, '6': 18, '7': 19, '8': 20, '9': 21, ':': 22, ';': 23, '=': 24, '?': 25, 'A': 26, 'B': 27, 'C': 28, 'D': 29, 'F': 30, 'G': 31, 'I': 32, 'K': 33, 'L': 34, 'M': 35, 'N': 36, 'O': 37, 'R': 38, 'S': 39, 'T': 40, 'U': 41, 'W': 42, 'Y': 43, '\\\\': 44, 'a': 45, 'b': 46, 'c': 47, 'd': 48, 'e': 49, 'f': 50, 'g': 51, 'h': 52, 'i': 53, 'j': 54, 'k': 55, 'l': 56, 'm': 57, 'n': 58, 'o': 59, 'p': 60, 'q': 61, 'r': 62, 's': 63, 't': 64, 'u': 65, 'v': 66, 'w': 67, 'x': 68, 'y': 69, 'z': 70, '\\xa0': 71}\n",
      "--------------------------\n",
      "{0: '\\t', 1: '\\n', 2: ' ', 3: '%', 4: '&', 5: \"'\", 6: '(', 7: ')', 8: ',', 9: '-', 10: '.', 11: '/', 12: '0', 13: '1', 14: '2', 15: '3', 16: '4', 17: '5', 18: '6', 19: '7', 20: '8', 21: '9', 22: ':', 23: ';', 24: '=', 25: '?', 26: 'A', 27: 'B', 28: 'C', 29: 'D', 30: 'F', 31: 'G', 32: 'I', 33: 'K', 34: 'L', 35: 'M', 36: 'N', 37: 'O', 38: 'R', 39: 'S', 40: 'T', 41: 'U', 42: 'W', 43: 'Y', 44: '\\\\', 45: 'a', 46: 'b', 47: 'c', 48: 'd', 49: 'e', 50: 'f', 51: 'g', 52: 'h', 53: 'i', 54: 'j', 55: 'k', 56: 'l', 57: 'm', 58: 'n', 59: 'o', 60: 'p', 61: 'q', 62: 'r', 63: 's', 64: 't', 65: 'u', 66: 'v', 67: 'w', 68: 'x', 69: 'y', 70: 'z', 71: '\\xa0'}\n"
     ]
    }
   ],
   "source": [
    "#Character to id, and id to character\n",
    "#dictionary that maps each character to a number and vice versa\n",
    "char2id = dict((c, i) for i, c in enumerate(chars))\n",
    "id2char = dict((i, c) for i, c in enumerate(chars))\n",
    "print(char2id)\n",
    "print('--------------------------')\n",
    "print(id2char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Given a probability of each character, return a likely character, one-hot encoded\n",
    "#our prediction will give us an array of probabilities of each character\n",
    "#we'll pick the most likely and one-hot encode it\n",
    "def sample(prediction):\n",
    "    #Samples are uniformly distributed over the half-open interval \n",
    "    r = random.uniform(0,1)\n",
    "    #store prediction char\n",
    "    s = 0\n",
    "    #since length > indices starting at 0\n",
    "    char_id = len(prediction) - 1\n",
    "    #for each char prediction probabilty\n",
    "    for i in range(len(prediction)):\n",
    "        #assign it to S\n",
    "        s += prediction[i]\n",
    "        #check if probability greater than our randomly generated one\n",
    "        if s >= r:\n",
    "            #if it is, thats the likely next char\n",
    "            char_id = i\n",
    "            break\n",
    "    #dont try to rank, just differentiate\n",
    "    #initialize the vector\n",
    "    char_one_hot = np.zeros(shape=[char_size])\n",
    "    #that characters ID encoded\n",
    "    #https://image.slidesharecdn.com/latin-150313140222-conversion-gate01/95/representation-learning-of-vectors-of-words-and-phrases-5-638.jpg?cb=1426255492\n",
    "    char_one_hot[char_id] = 1.0\n",
    "    return char_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#vectorize our data to feed it into model\n",
    "\n",
    "len_per_section = 100\n",
    "skip = 1\n",
    "sections = []\n",
    "next_chars = []\n",
    "#fill sections list with chunks of text, every 2 characters create a new 50 \n",
    "#character long section\n",
    "#because we are generating it at a character level\n",
    "for i in range(0, len(text) - len_per_section, skip):\n",
    "    sections.append(text[i: i + len_per_section])\n",
    "    next_chars.append(text[i + len_per_section])\n",
    "#Vectorize input and output\n",
    "#matrix of section length by num of characters\n",
    "X = np.zeros((len(sections), len_per_section, char_size))\n",
    "#label column for all the character id's, still zero\n",
    "y = np.zeros((len(sections), char_size))\n",
    "#for each char in each section, convert each char to an ID\n",
    "#for each section convert the labels to ids \n",
    "for i, section in enumerate(sections):\n",
    "    for j, char in enumerate(section):\n",
    "        X[i, j, char2id[char]] = 1\n",
    "    y[i, char2id[next_chars[i]]] = 1\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 72)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape\n",
    "X[0].shape\n",
    "y.shape\n",
    "y[0].shape\n",
    "X[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#The higher the batch size, the more memory space you'll need.\n",
    "#if you have 1000 training examples, \n",
    "#and your batch size is 500, then it will take 2 iterations to complete 1 epoch.\n",
    "batch_size = 128\n",
    "#total iterations\n",
    "max_steps = 10000\n",
    "#how often to log?\n",
    "log_every = 1000\n",
    "#how often to save?\n",
    "save_every = 1000\n",
    "#too few and underfitting\n",
    "#Underfitting occurs when there are too few neurons \n",
    "#in the hidden layers to adequately detect the signals in a complicated data set.\n",
    "#too many and overfitting\n",
    "hidden_nodes = 100\n",
    "\n",
    "print('training data size:', len(X))\n",
    "print('approximate steps per epoch:', int(len(X)/batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(X.shape)\n",
    "# print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# checkpoint_path = \"training_2/cp-{epoch:04d}.ckpt\"\n",
    "# checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "#     checkpoint_path, verbose=1, save_weights_only=True,\n",
    "#     # Save weights, every 5-epochs.\n",
    "#     period=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define the checkpoint\n",
    "filepath=\"/home/leena/Sohel/training_lstm_wieghts/weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min',period=1000)\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "model.fit(X, y,batch_size=128,epochs=10000,verbose=2,callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('/home/leena/LSTM/weights-lstm-34-0.1095.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentence = 'I plan to make the world a better place '\n",
    "\n",
    "def random_text_seed_and_generation():\n",
    "\n",
    "\n",
    "    start_index = random.randint(0, len(text) - 100 - 1)\n",
    "    test_generated = ''\n",
    "    sentence = text[start_index: start_index + 100]\n",
    "    test_generated+= sentence\n",
    "    print('----- Generating with seed: \"' + sentence + '\"')\n",
    "    sys.stdout.write(test_generated)\n",
    "    test_generated = sentence\n",
    "\n",
    "    for i in range(500):\n",
    "\n",
    "        x_pred = np.zeros((1,len_per_section,len(chars)))\n",
    "        for t, char in enumerate(sentence):\n",
    "\n",
    "            x_pred[0,t,char2id[char]] = 1.\n",
    "\n",
    "\n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        #one hot encode it\n",
    "        next_char_one_hot = sample(preds)\n",
    "        #get the indices of the max values (highest probability)  and convert to char\n",
    "        next_char = id2char[np.argmax(next_char_one_hot)]\n",
    "        #add each char to the output text iteratively\n",
    "        test_generated += next_char\n",
    "        #update the\n",
    "        test_X = next_char_one_hot.reshape((1, char_size))\n",
    "    #a = json.dumps(test_generated)\n",
    "\n",
    "    return test_generated\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Generating with seed: \"an\\n Blue suitBlue woven blazer in a self-striped pattern, has a notched lapel, single-breasted with\"\n",
      "an\\n Blue suitBlue woven blazer in a self-striped pattern, has a notched lapel, single-breasted with"
     ]
    }
   ],
   "source": [
    "nc = random_text_seed_and_generation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72,)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ncoh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Y'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def user_input_text_generation(sentence):\n",
    "    \n",
    "    \n",
    "    test_generated = sentence\n",
    "\n",
    "    for i in range(500):\n",
    "\n",
    "        x_pred = np.zeros((1,50,len(chars)))\n",
    "        for t, char in enumerate(sentence):\n",
    "\n",
    "            x_pred[0,t,char2id[char]] = 1.\n",
    "\n",
    "\n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        #one hot encode it\n",
    "        next_char_one_hot = sample(preds)\n",
    "        #get the indices of the max values (highest probability)  and convert to char\n",
    "        next_char = id2char[np.argmax(next_char_one_hot)]\n",
    "        #add each char to the output text iteratively\n",
    "        test_generated += next_char\n",
    "        #update the \n",
    "        test_X = next_char_one_hot.reshape((1, char_size))\n",
    "\n",
    "    return test_generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
